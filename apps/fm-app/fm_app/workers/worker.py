import asyncio
import logging
import warnings
from datetime import date, datetime
from decimal import Decimal
from logging.config import dictConfig

import structlog
import urllib3
from celery import Celery
from celery.signals import setup_logging
from celery.utils.log import get_task_logger

# from pydantic import ValidationError
from sqlalchemy import create_engine
from sqlalchemy.orm import sessionmaker
from trino.auth import BasicAuthentication

# Disable urllib3 SSL warnings for Trino connections with verify=False
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)


def serialize_value(value):
    """Convert non-JSON-serializable types to JSON-compatible formats."""
    if isinstance(value, (date, datetime)):
        return value.isoformat()
    elif isinstance(value, Decimal):
        return float(value)
    elif value is None:
        return None
    else:
        return value


from fm_app.ai_models.llm import AnthropicModel, DeepSeekModel, GeminiModel, OpenAIModel
from fm_app.api.db_session import normalize_database_driver
from fm_app.api.model import (
    AddRequestModel,
    DBType,
    FlowType,
    InteractiveRequestType,
    ModelType,
    RequestStatus,
    UpdateRequestModel,
    WorkerRequest,
)
from fm_app.config import get_settings
from fm_app.db.db import add_request, update_request, update_request_failure
from fm_app.stopwatch import stopwatch
from fm_app.workers.db_session import get_db
from fm_app.workers.experimental.agent import close_agent, init_agent
from fm_app.workers.experimental.flex_flow import flex_flow
from fm_app.workers.experimental.langgraph_flow import langgraph_flow
from fm_app.workers.experimental.mcp_flow import mcp_flow
from fm_app.workers.interactive_flow import interactive_flow
from fm_app.workers.legacy.data_only_flow import data_only_flow
from fm_app.workers.legacy.multistep_flow import multistep_flow
from fm_app.workers.legacy.simple_flow import simple_flow

settings = get_settings()

LOGGING_CONFIG_NORMAL = {
    "version": 1,
    "disable_existing_loggers": False,
    "formatters": {
        "plain": {
            # "()": jsonlogger.JsonFormatter,
            "format": "%(asctime)s %(name)s %(levelname)s %(message)s",
        }
    },
    "handlers": {
        "default": {
            "level": settings.log_level,
            "class": "logging.StreamHandler",
            "formatter": "plain",
        },
    },
    "loggers": {
        "": {"handlers": ["default"], "level": settings.log_level},
        "celery.app.trace": {
            "handlers": ["default"],
            "level": "WARNING",
            "propagate": False,
        },
        "celery.worker": {
            "handlers": ["default"],
            "level": "WARNING",
            "propagate": False,
        },
        "amqp": {
            "handlers": ["default"],
            "level": "WARNING",
            "propagate": False,
        },
    },
}
LOGGING_CONFIG_JSON = {
    "version": 1,
    "disable_existing_loggers": False,
    "formatters": {
        "plain": {
            # "()": jsonlogger.JsonFormatter,
            "format": "%(message)s",
        }
    },
    "handlers": {
        "default": {
            "level": settings.log_level,
            "class": "logging.StreamHandler",
            "formatter": "plain",
        },
    },
    "loggers": {
        "": {"handlers": ["default"], "level": settings.log_level},
        "celery.app.trace": {
            "handlers": ["default"],
            "level": "WARNING",
            "propagate": False,
        },
        "celery.worker": {
            "handlers": ["default"],
            "level": "WARNING",
            "propagate": False,
        },
        "amqp": {
            "handlers": ["default"],
            "level": "WARNING",
            "propagate": False,
        },
    },
}


def create_wh_engine(driver: str, url: str):
    if driver == "trino":
        logging.info("Starting Trino session")
        wh_engine = create_engine(
            url,
            echo=False,  # Disable SQLAlchemy query logging
            pool_size=5,
            max_overflow=10,
            pool_pre_ping=True,
            pool_recycle=360,
            connect_args={
                "http_scheme": "https",
                "verify": False,  # use a CA file path instead in prod, e.g. "/path/to/ca.crt"
                "auth": BasicAuthentication(
                    settings.database_wh_user, settings.database_wh_pass
                ),
            },
        )
    else:
        logging.info(f"Starting {driver} session")
        wh_engine = create_engine(
            url,
            pool_size=10,
            max_overflow=20,
            pool_pre_ping=True,
            pool_recycle=360,
        )
    return wh_engine


app = Celery(
    "ai_handler",
    broker=settings.wrk_broker_connection,
)

# Use PostgreSQL as result backend
result_backend_url = f"db+postgresql://{settings.database_user}:{settings.database_pass}@{settings.database_server}:{settings.database_port}/{settings.database_db}"

app.conf.update(
    broker_connection_retry_on_startup=True,
    result_backend=result_backend_url,
    result_expires=3600,  # Results expire after 1 hour
)

normalized_driver = normalize_database_driver(settings.database_wh_driver)

DATABASE_URL_WH = f"{settings.database_wh_driver}://{settings.database_wh_user}:{settings.database_wh_pass}@{settings.database_wh_server}:{settings.database_wh_port}/{settings.database_wh_db}{settings.database_wh_params}"
DATABASE_URL_WH_NEW = f"{settings.database_wh_driver}://{settings.database_wh_user}:{settings.database_wh_pass}@{settings.database_wh_server_new}:{settings.database_wh_port_new}/{settings.database_wh_db_new}{settings.database_wh_params_new}"
DATABASE_URL_WH_V2 = f"{settings.database_wh_driver}://{settings.database_wh_user}:{settings.database_wh_pass}@{settings.database_wh_server_v2}:{settings.database_wh_port_v2}/{settings.database_wh_db_v2}{settings.database_wh_params_v2}"
ENGINE_WH = create_wh_engine(normalized_driver, DATABASE_URL_WH)
# ENGINE_WH = create_engine(
#     DATABASE_URL_WH, pool_size=40, max_overflow=60, pool_pre_ping=True, pool_recycle=360
# )
# )
ENGINE_WH_NEW = create_wh_engine(normalized_driver, DATABASE_URL_WH_NEW)
# ENGINE_WH_NEW = create_engine(
#     DATABASE_URL_WH_NEW,
#     pool_size=40,
#     max_overflow=60,
#     pool_pre_ping=True,
#     pool_recycle=360,
# )

ENGINE_WH_V2 = create_wh_engine(normalized_driver, DATABASE_URL_WH_V2)
# ENGINE_WH_V2 = create_engine(
#     DATABASE_URL_WH_V2,
#     pool_size=40,
#     max_overflow=60,
#     pool_pre_ping=True,
#     pool_recycle=360,
# )

SESSION_WH = sessionmaker(bind=ENGINE_WH, expire_on_commit=False)
SESSION_WH_NEW = sessionmaker(bind=ENGINE_WH_NEW, expire_on_commit=False)
SESSION_WH_V2 = sessionmaker(bind=ENGINE_WH_V2, expire_on_commit=False)

loop = asyncio.new_event_loop()
asyncio.set_event_loop(loop)


def add_fields_to_log(logger, log_method, event_dict):
    if isinstance(logger, logging.Logger):
        event_dict["name"] = logger.name
    ts = event_dict.get("timestamp")
    if ts:
        event_dict["asctime"] = ts
    return event_dict


if settings.json_log:
    structlog.configure(
        processors=[
            structlog.contextvars.merge_contextvars,
            structlog.processors.add_log_level,
            structlog.processors.TimeStamper(fmt="iso"),
            add_fields_to_log,
            structlog.processors.JSONRenderer(),
        ],
    )
logger = structlog.wrap_logger(get_task_logger(__name__))


@setup_logging.connect
def config_loggers(*args, **kwargs):
    if settings.json_log:
        dictConfig(LOGGING_CONFIG_JSON)
    else:
        dictConfig(LOGGING_CONFIG_NORMAL)


@app.on_after_finalize.connect
def setup_agent_context(sender, **kwargs):
    # Run the agent initializer once on worker startup
    # asyncio.get_event_loop().run_until_complete(init_agent())
    logger.info("Agent context setup placeholder")


@app.on_after_finalize.disconnect
def cleanup_agent_context(sender, **kwargs):
    # Run the agent initializer once on worker startup
    # asyncio.get_event_loop().run_until_complete(close_agent())
    logger.info("Agent context setup placeholder")


@app.task(name="wrk_add_request")
def wrk_add_request(args):
    return asyncio.get_event_loop().run_until_complete(_wrk_add_request(args))


async def _wrk_add_request(args):
    request = WorkerRequest(**args)
    try:
        async for db in get_db():
            db_wh = SESSION_WH()
            db_wh_new = SESSION_WH_NEW()
            db_wh_v2 = SESSION_WH_V2()
            logger.info(
                "Got request",
                args=args,
                flow_step_num=0,
                flow_stage="got_request",
                flow=request.flow,
                model=request.model,
                db=request.db,
            )

            # new flows
            if request.model and (request.db or request.db == ""):
                if request.model == ModelType.openai_default:
                    OpenAIModel.init(settings)
                    llm = OpenAIModel
                elif request.model == ModelType.gemini_default:
                    GeminiModel.init(settings)
                    llm = GeminiModel
                elif request.model == ModelType.deepseek_default:
                    DeepSeekModel.init(settings)
                    llm = DeepSeekModel
                elif request.model == ModelType.anthropic_default:
                    AnthropicModel.init(settings)
                    llm = AnthropicModel
                else:
                    raise NotImplementedError("model not known or not implemented")

                if request.db == DBType.legacy:
                    db_wh = db_wh
                elif request.db == DBType.new_wh:
                    db_wh = db_wh_new
                elif request.db == DBType.v2:
                    db_wh = db_wh_v2
                else:
                    raise NotImplementedError("db not known or not implemented")

                if request.flow == FlowType.simple:
                    request = await simple_flow(request, llm, db_wh=db_wh, db=db)
                elif request.flow == FlowType.multistep:
                    request = await multistep_flow(request, llm, db_wh=db_wh, db=db)
                elif request.flow == FlowType.data_only:
                    request = await data_only_flow(request, llm, db_wh=db_wh, db=db)
                elif request.flow == FlowType.mcp:
                    request = await mcp_flow(request, llm)
                elif request.flow == FlowType.flex:
                    request = await flex_flow(request, llm, db_wh=db_wh, db=db)
                elif request.flow == FlowType.langgraph:
                    request = await langgraph_flow(request, llm, db_wh=db_wh, db=db)
                elif request.flow == FlowType.interactive:
                    request = await interactive_flow(request, llm, db_wh=db_wh, db=db)
                else:
                    raise NotImplementedError("flow not known or not implemented")

            # legacy flows
            elif request.flow == FlowType.openai_simple:
                OpenAIModel.init(settings)  # Ensure client is initialized
                request = await simple_flow(request, OpenAIModel, db_wh=db_wh, db=db)
            elif request.flow == FlowType.openai_simple_new_wh:
                OpenAIModel.init(settings)  # Ensure client is initialized
                request = await simple_flow(
                    request, OpenAIModel, db_wh=db_wh_new, db=db
                )
            elif request.flow == FlowType.openai_simple_v2:
                OpenAIModel.init(settings)  # Ensure client is initialized
                request = await simple_flow(request, OpenAIModel, db_wh=db_wh_v2, db=db)
            elif request.flow == FlowType.openai_multisteps:
                OpenAIModel.init(settings)  # Ensure client is initialized
                request = await multistep_flow(request, OpenAIModel, db_wh=db_wh, db=db)
            elif request.flow == FlowType.deepseek_simple:
                DeepSeekModel.init(settings)  # Ensure client is initialized
                request = await simple_flow(request, DeepSeekModel, db_wh=db_wh, db=db)
            elif request.flow == FlowType.deepseek_simple_new_wh:
                DeepSeekModel.init(settings)  # Ensure client is initialized
                request = await simple_flow(
                    request, DeepSeekModel, db_wh=db_wh_new, db=db
                )
            elif request.flow == FlowType.deepseek_simple_v2:
                DeepSeekModel.init(settings)  # Ensure client is initialized
                request = await simple_flow(
                    request, DeepSeekModel, db_wh=db_wh_v2, db=db
                )
            elif request.flow == FlowType.deepseek_multistep:
                DeepSeekModel.init(settings)  # Ensure client is initialized
                request = await multistep_flow(
                    request, DeepSeekModel, db_wh=db_wh, db=db
                )
            elif request.flow == FlowType.gemini_simple:
                GeminiModel.init(settings)  # Ensure client is initialized
                request = await simple_flow(request, GeminiModel, db_wh=db_wh, db=db)
            elif request.flow == FlowType.gemini_simple_new_wh:
                GeminiModel.init(settings)  # Ensure client is initialized
                request = await simple_flow(
                    request, GeminiModel, db_wh=db_wh_new, db=db
                )
            elif request.flow == FlowType.gemini_simple_v2:
                GeminiModel.init(settings)  # Ensure client is initialized
                request = await simple_flow(request, GeminiModel, db_wh=db_wh_v2, db=db)
            elif request.flow == FlowType.gemini_multistep:
                GeminiModel.init(settings)  # Ensure client is initialized
                request = await multistep_flow(request, GeminiModel, db_wh=db_wh, db=db)
            elif request.flow == FlowType.anthropic_simple:
                AnthropicModel.init(settings)  # Ensure client is initialized
                request = await simple_flow(request, AnthropicModel, db_wh=db_wh, db=db)
            elif request.flow == FlowType.anthropic_simple_new_wh:
                AnthropicModel.init(settings)  # Ensure client is initialized
                request = await simple_flow(
                    request, AnthropicModel, db_wh=db_wh_new, db=db
                )
            elif request.flow == FlowType.anthropic_simple_v2:
                AnthropicModel.init(settings)  # Ensure client is initialized
                request = await simple_flow(
                    request, AnthropicModel, db_wh=db_wh_v2, db=db
                )
            elif request.flow == FlowType.anthropic_multistep:
                AnthropicModel.init(settings)  # Ensure client is initialized
                request = await multistep_flow(
                    request, AnthropicModel, db_wh=db_wh, db=db
                )
            else:
                raise NotImplementedError("leg.flow not known or not implemented")

            if request.status == RequestStatus.error:
                logger.error(
                    "Error in flow",
                    request=request.model_dump(),
                    flow_stage="error_in_flow",
                    flow_step_num=10000,
                )
            else:
                logger.info(
                    "Done with request",
                    request=request.model_dump(),
                    flow_stage="done_with_request",
                    flow_step_num=10000,
                )

                status = (
                    RequestStatus.done
                    if request.status != RequestStatus.error
                    else RequestStatus.error
                )
                structured_response = request.structured_response
                if structured_response is None:
                    await update_request(
                        db=db,
                        update=UpdateRequestModel(
                            request_id=request.request_id,
                            err=request.err,
                            status=status,
                            response=request.response,
                        ),
                    )
                else:
                    # if structured_response.linked_session_id is not None:
                    #    # launch a new worker task for linked session
                    #    (response, task_id) = await add_request(
                    #        user_owner=request.user,
                    #        session_id=structured_response.linked_session_id,
                    #        add_req=AddRequestModel(
                    #            request=request.request,
                    #            request_type=InteractiveRequestType.tbd,
                    #            flow=request.flow,
                    #            model=request.model,
                    #            db=request.db,
                    #            refs=request.refs,
                    #        ),
                    #        db=db,
                    #    )
                    #    wrk_req = WorkerRequest(
                    #        session_id=structured_response.linked_session_id,
                    #        request_id=response.request_id,
                    #        user=request.user,
                    #        request=request.request,
                    #        request_type=InteractiveRequestType.tbd,
                    #        response=None,
                    #        status=RequestStatus.new,
                    #        flow=request.flow,
                    #        model=request.model,
                    #        db=request.db,
                    #        refs=request.refs,
                    #    )
                    #    wrk_arg = wrk_req.model_dump()
                    #    task = wrk_add_request.apply_async(
                    #        args=[wrk_arg], task_id=task_id
                    #    )
                    #    logging.info(
                    #        "Send linked task",
                    #        extra={"action": "send_task", "task_id": task},
                    #    )
                    #    print("spawned linked task", task_id)

                    await update_request(
                        db=db,
                        update=UpdateRequestModel(
                            request_id=request.request_id,
                            err=request.err,
                            status=status,
                            response=request.response,
                            sql=structured_response.sql,
                            intent=structured_response.intent,
                            assumptions=structured_response.assumptions,
                            intro=structured_response.intro,
                            outro=structured_response.outro,
                            raw_data_labels=structured_response.raw_data_labels,
                            raw_data_rows=structured_response.raw_data_rows,
                            csv=structured_response.csv,
                            chart=structured_response.chart,
                            chart_url=structured_response.chart_url,
                            refs=(
                                structured_response.refs.model_dump()
                                if structured_response.refs
                                else None
                            ),
                            linked_session_id=structured_response.linked_session_id,
                        ),
                    )
            # await db.close()

    except Exception as e:
        async for db in get_db():
            request.status = RequestStatus.error
            logger.error(
                f"Unhandled Exception: {e}", request=request.model_dump(), exc_info=True
            )
            request.err = "Unhandled exception, check logs"
            await update_request_failure(err=str(e), status=RequestStatus.error, db=db)
            # await db.close()

    # finally:
    #    return request


@app.task(name="wrk_fetch_data", bind=True, soft_time_limit=300, time_limit=600)
def wrk_fetch_data(self, args):
    """
    Background task for fetching data from warehouse.
    Args:
        args: dict with keys:
            - query_id: str (UUID)
            - sql: str (the SQL query to execute)
            - limit: int
            - offset: int
            - sort_by: Optional[str]
            - sort_order: str
    Returns:
        dict with keys:
            - status: "success" | "error"
            - rows: list[dict] (if success)
            - total_rows: int (if success)
            - error: str (if error)
    """
    from sqlalchemy import text

    query_id = args.get("query_id")
    sql = args.get("sql")
    limit = args.get("limit", 100)
    offset = args.get("offset", 0)
    sort_by = args.get("sort_by")
    sort_order = args.get("sort_order", "asc")

    logger.debug(
        "Fetching data",
        query_id=query_id,
        limit=limit,
        offset=offset,
        sort_by=sort_by,
    )

    try:
        # Fetch actual data
        from fm_app.api.routes import build_sorted_paginated_sql

        # Build the paginated SQL (no need for total_count now)
        combined_sql = build_sorted_paginated_sql(
            sql,
            sort_by=sort_by,
            sort_order=sort_order,
            include_total_count=settings,  # We already have it
        )

        # Execute using the warehouse engine
        with ENGINE_WH_V2.connect() as conn:
            result = conn.execute(
                text(combined_sql),
                {
                    "limit": limit,
                    "offset": offset,
                },
            )

            # Convert to dicts
            columns = result.keys()
            rows = [dict(zip(columns, row)) for row in result.fetchall()]

            # Extract total_count if present (case-insensitive for Trino)
            if rows:
                total_count = None
                for k in rows[0].keys():
                    if k.lower() == "total_count":
                        total_count = rows[0].get(k, 0)
                        break
                if total_count is None:
                    total_count = 0
            else:
                total_count = 0

            logger.debug(
                "Fetching data done",
                query_id=query_id,
                total_count=total_count,
            )

            return {
                "status": "success",
                "query_id": query_id,
                "rows": [
                    {
                        k: serialize_value(v)
                        for k, v in row.items()
                        if k.lower() != "total_count"
                    }
                    for row in rows
                ],
                "total_rows": total_count,
                "limit": limit,
                "offset": offset,
            }

    except Exception as e:
        from celery.exceptions import SoftTimeLimitExceeded

        if isinstance(e, SoftTimeLimitExceeded):
            logger.warning(
                f"Query timeout (5 minute soft limit): {query_id}",
                query_id=query_id,
            )
            return {
                "status": "error",
                "query_id": query_id,
                "error": "Query execution timed out (5 minute limit). Please simplify your query or add more filters.",
            }

        logger.error(
            f"Error fetching data: {e}",
            query_id=query_id,
            exc_info=True,
        )
        return {
            "status": "error",
            "query_id": query_id,
            "error": str(e),
        }
